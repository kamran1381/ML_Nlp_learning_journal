{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b057fad5",
      "metadata": {
        "id": "b057fad5"
      },
      "source": [
        "# **Practice Assignment: NLP with NLTK & spaCy**\n",
        "\n",
        "* This assignment is part of the NLP Workshop on YouTube, which is free and open to the public.\n",
        "* **Lecturer: Reza Shokrzad.**\n",
        "*‚Äå [ÿØÿ≥ÿ™ÿ±ÿ≥€å ÿ®Ÿá ÿ¨ŸÑÿ≥Ÿá ÿßŸàŸÑ ⁄©ŸÑÿßÿ≥](https://youtube.com/live/lDCoqQSc4ZE?feature=share)\n",
        "* [ÿ®ÿ±ŸÜÿßŸÖŸá ÿßÿ¨ÿ±ÿß€å€å ⁄©ŸÑÿßÿ≥ Ÿà ÿ¨ŸÑÿ≥ÿßÿ™](https://docs.google.com/spreadsheets/d/1SP3NJ9H7yp8sgof-zp_t4oxmdxjMdEgoL_mmCDvdUm4/edit?gid=0#gid=0)\n",
        "\n",
        "salam\n",
        "Welcome to this **Fill-in-the-Blanks NLP Assignment!** üéØ This exercise will help you solidify your understanding of **NLTK** and **spaCy** by filling in the missing parts of the code. Follow the instructions carefully, and make sure to test your solutions!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa085b7f",
      "metadata": {
        "id": "aa085b7f"
      },
      "source": [
        "## **1. Working with Corpora & Lexical Resources**\n",
        "**Task:** Load and analyze texts from different corpora.\n",
        "- Use NLTK‚Äôs **Gutenberg** corpus to load the text of *Moby Dick*.\n",
        "- Tokenize it into words.\n",
        "- Count the top 10 most frequent words (excluding stopwords)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "757d9099",
      "metadata": {
        "id": "757d9099",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e6f5cdd-8d63-4660-f639-fc0ee9385ff9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('whale', 1095), ('one', 913), ('like', 580), ('upon', 565), ('ahab', 511), ('man', 498), ('ship', 469), ('old', 443), ('ye', 438), ('would', 436)]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "# the first step actually involves fiding out what kinds of text there are in this\n",
        "fileids = nltk.corpus.gutenberg.fileids()\n",
        "\n",
        "fileid = \"melville-moby_dick.txt\"\n",
        "# Load text\n",
        "text = gutenberg.raw(fileid)  # FILL THIS\n",
        "\n",
        "# Tokenize words\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stopwords.words('english')]  # FILL THIS\n",
        "\n",
        "# Compute frequency distribution\n",
        "fdist = FreqDist(filtered_words)\n",
        "#aalam sonya\n",
        "# Print top 10 words\n",
        "print(fdist.most_common(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2894d7a",
      "metadata": {
        "id": "f2894d7a"
      },
      "source": [
        "## **2. Tokenization Techniques**\n",
        "**Task:** Tokenize a given text using both **NLTK** and **spaCy**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85a9ac50",
      "metadata": {
        "id": "85a9ac50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0ba4f98-f15e-479f-c1b2-3da0acee5a70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Word Tokens: ['SpaCy', 'is', 'fast', '!', 'However', ',', 'NLTK', 'provides', 'flexibility', 'in', 'tokenization', '.']\n",
            "NLTK Sentence Tokens: ['SpaCy is fast!', 'However, NLTK provides flexibility in tokenization.']\n",
            "spaCy Tokens: ['SpaCy', 'is', 'fast', '!', 'However', ',', 'NLTK', 'provides', 'flexibility', 'in', 'tokenization', '.']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "text = \"SpaCy is fast! However, NLTK provides flexibility in tokenization.\"\n",
        "\n",
        "# NLTK Tokenization\n",
        "nltk_word_tokens = word_tokenize(text)  # FILL THIS\n",
        "nltk_sent_tokens = sent_tokenize(text)  # FILL THIS\n",
        "\n",
        "# spaCy Tokenization\n",
        "doc = nlp(text)\n",
        "spacy_tokens = [token.text for token in doc]\n",
        "\n",
        "print(\"NLTK Word Tokens:\", nltk_word_tokens)\n",
        "print(\"NLTK Sentence Tokens:\", nltk_sent_tokens)\n",
        "print(\"spaCy Tokens:\", spacy_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Regex Pattern Matching for Phone Number Detection**\n",
        "**Task:** Write a pattern using regex to find the phone number in the text."
      ],
      "metadata": {
        "id": "Sp7RCE4fIaVV"
      },
      "id": "Sp7RCE4fIaVV"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Example 2: Phone Number Extraction\n",
        "text_phones = \"Call me at +1-202-555-0173 or reach our office at (415) 123-4567.\"\n",
        "phone_pattern = r\"(\\+?\\d{1,2}[-\\s]?)?(\\(?\\d{3}\\)?)[-\\s]?\\d{3}[-\\s]?\\d{4}\"\n",
        "\n",
        "phones = re.findall(phone_pattern , text_phones)\n",
        "print(\"Detected Phone Numbers:\", phones)\n"
      ],
      "metadata": {
        "id": "XeSzDZ2-Icdv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f30395ae-6483-4942-9cbc-39e2cf6c9c76"
      },
      "id": "XeSzDZ2-Icdv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected Phone Numbers: [('+1-', '202'), ('', '(415)')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. **Stopwords Filtering using NLTK**\n",
        "**Task:** Analyze movie reviews where stopwords are removed to focus on meaningful words."
      ],
      "metadata": {
        "id": "pdn94C-1cGgc"
      },
      "id": "pdn94C-1cGgc"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# nltk.download(\"stopwords\")\n",
        "# nltk.download(\"punkt\")\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# üé¨ Sample Movie Review\n",
        "review = \"\"\"The movie was absolutely amazing! The cinematography was stunning, and the characters were incredibly well-developed.\n",
        "However, the storyline felt a bit predictable at times, and some scenes were unnecessarily long. Overall, a great experience!\"\"\"\n",
        "\n",
        "# Tokenize words\n",
        "words = word_tokenize(review)\n",
        "#sla,\n",
        "# Remove stopwords\n",
        "filtered_words = [word for word in words if word.lower() not in stopwords.words(\"english\") and word.isalpha()]\n",
        "\n",
        "# Output results\n",
        "print(\"Original Words:\", words)\n",
        "print(\"\\nFiltered (No Stopwords):\", filtered_words)\n"
      ],
      "metadata": {
        "id": "qFGnnFOjcM23"
      },
      "id": "qFGnnFOjcM23",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. **Stemming Methods using NLTK**\n",
        "**Task:** Analyze legal and scientific terms to observe how different stemming algorithms behave."
      ],
      "metadata": {
        "id": "7UezErpHcu3x"
      },
      "id": "7UezErpHcu3x"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "\n",
        "# ‚öñÔ∏è Sample Legal & Scientific Terms\n",
        "words = [\"arguing\", \"justification\", \"liable\", \"obligations\", \"classification\", \"microbiology\", \"evolutionary\", \"running\", \"happiness\"]\n",
        "\n",
        "# Initialize Stemmer Objects\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "\n",
        "# Apply Stemming\n",
        "porter_stems = [porter.stem(word) for word in words]\n",
        "lancaster_stems = [lancaster.stem(word) for word in words]\n",
        "\n",
        "# Output Results\n",
        "print(\"Original Words:\", words)\n",
        "print(\"\\nPorter Stemmer Results:\", porter_stems)\n",
        "print(\"\\nLancaster Stemmer Results:\", lancaster_stems)\n"
      ],
      "metadata": {
        "id": "Kpvzkd31d6Na",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61c69e3a-0a75-4d4e-b799-1a475ec0535e"
      },
      "id": "Kpvzkd31d6Na",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['arguing', 'justification', 'liable', 'obligations', 'classification', 'microbiology', 'evolutionary', 'running', 'happiness']\n",
            "\n",
            "Porter Stemmer Results: ['argu', 'justif', 'liabl', 'oblig', 'classif', 'microbiolog', 'evolutionari', 'run', 'happi']\n",
            "\n",
            "Lancaster Stemmer Results: ['argu', 'just', 'liabl', 'oblig', 'class', 'microbiolog', 'evolv', 'run', 'happy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. **Lemmatization Strategies using NLTK & spaCy**\n",
        "\n",
        "### NLTK‚Äôs WordNetLemmatizer\n",
        "**Task:** Lemmatize a political news headline to show how lemmatization helps retain the correct part of speech (POS) while normalizing words."
      ],
      "metadata": {
        "id": "ffIhEdRJeoI4"
      },
      "id": "ffIhEdRJeoI4"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# üì∞ Sample News Headline\n",
        "headline = \"The senators debated the increasing regulations affecting technology companies.\"\n",
        "\n",
        "# Tokenize words\n",
        "words = word_tokenize(headline)\n",
        "\n",
        "# Initialize Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply Lemmatization (default without POS tagging)\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print(\"Original Words:\", words)\n",
        "print(\"\\nLemmatized Words:\", lemmatized_words)\n"
      ],
      "metadata": {
        "id": "o4LlZqSxe086",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a6a032-c87b-430d-b596-a27d4ff7ec4e"
      },
      "id": "o4LlZqSxe086",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['The', 'senators', 'debated', 'the', 'increasing', 'regulations', 'affecting', 'technology', 'companies', '.']\n",
            "\n",
            "Lemmatized Words: ['The', 'senator', 'debated', 'the', 'increasing', 'regulation', 'affecting', 'technology', 'company', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### spaCy‚Äôs Built-in Lemmatizer"
      ],
      "metadata": {
        "id": "C61P_pdvfJlH"
      },
      "id": "C61P_pdvfJlH"
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process the same headline\n",
        "doc = nlp(headline)\n",
        "\n",
        "# Apply Lemmatization\n",
        "spacy_lemmatized = [token.text for token in doc]\n",
        "\n",
        "print(\"\\nspaCy Lemmatized Words:\", spacy_lemmatized)\n"
      ],
      "metadata": {
        "id": "4qPNWbqafWt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8446b20d-02cd-4139-9baa-255d90e777c2"
      },
      "id": "4qPNWbqafWt3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "spaCy Lemmatized Words: ['The', 'senators', 'debated', 'the', 'increasing', 'regulations', 'affecting', 'technology', 'companies', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. **Parsing & Chunking using NLTK**\n",
        "\n",
        "**Task:** Analyze legal contracts and job descriptions where parsing and chunking help extract meaningful phrases like noun phrases (NPs) or verb phrases (VPs)."
      ],
      "metadata": {
        "id": "nf-YB5yjfdl5"
      },
      "id": "nf-YB5yjfdl5"
    },
    {
      "cell_type": "code",
      "source": [
        "# üìú Task: Extracting Key Phrases from Legal & Job Documents\n",
        "import nltk\n",
        "\n",
        "# nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
        "\n",
        "# üìú Sample Legal Contract Text\n",
        "contract_text = \"The tenant shall pay the monthly rent before the 5th of each month.\"\n",
        "\n",
        "# Tokenize & POS Tagging\n",
        "words = nltk.word_tokenize(contract_text)\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "# Define a Chunking Grammar for Noun Phrases (NP)\n",
        "grammar = r\"NP: {<DT>?<JJ>*<NN>+}\"\n",
        "\n",
        "# Apply Chunking\n",
        "chunk_parser = nltk.RegexpParser(grammar)\n",
        "tree = chunk_parser.parse(pos_tags)\n",
        "\n",
        "# Display Results\n",
        "print(\"Chunked Tree:\")\n",
        "tree.pretty_print()\n"
      ],
      "metadata": {
        "id": "t9e4fOIuft4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaeea0be-976b-4e1c-dbee-3865b728f54c"
      },
      "id": "t9e4fOIuft4n",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunked Tree:\n",
            "                                               S                                                                     \n",
            "    ___________________________________________|__________________________________________________________            \n",
            "   |       |        |       |      |      |    |          NP                      NP                      NP         \n",
            "   |       |        |       |      |      |    |     _____|______         ________|_________         _____|_____      \n",
            "shall/MD pay/VB before/IN the/DT 5th/CD of/IN ./. The/DT     tenant/NN the/DT monthly/JJ rent/NN each/DT     month/NN\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. **Exploring Hyponyms & Hypernyms using WordNet (NLTK)**\n",
        "\n",
        "**Task:** Hyponyms (specific terms) and hypernyms (general terms) in scientific and business domains, where hierarchical relationships between words are essential."
      ],
      "metadata": {
        "id": "GTmtGRjofqpB"
      },
      "id": "GTmtGRjofqpB"
    },
    {
      "cell_type": "code",
      "source": [
        "# üîç Task: Explore Word Relationships in Science & Business\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# ü¶Å Find Hypernyms & Hyponyms for \"lion\"\n",
        "word = \"lion\"\n",
        "synset = wordnet.synsets(word)[0]  # Selecting the first synset\n",
        "\n",
        "# Hypernyms (More General Category)\n",
        "hypernyms = synset.hypernyms()\n",
        "print(f\"Hypernyms (More General Concept) of '{word}':\")\n",
        "print([hypernym.name().split('.')[0] for hypernym in hypernyms])\n",
        "\n",
        "# Hyponyms (More Specific Types)\n",
        "hyponyms = synset.hyponyms()\n",
        "print(f\"\\nHyponyms (More Specific Types) of '{word}':\")\n",
        "print([hyponym.name().split('.')[0] for hyponym in hyponyms])\n"
      ],
      "metadata": {
        "id": "EELbOY0dgjsK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "358e061a-ca40-4e74-9530-418274607adf"
      },
      "id": "EELbOY0dgjsK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hypernyms (More General Concept) of 'lion':\n",
            "['big_cat']\n",
            "\n",
            "Hyponyms (More Specific Types) of 'lion':\n",
            "['lionet', 'lioness', 'lion_cub']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdb5a06b",
      "metadata": {
        "id": "fdb5a06b"
      },
      "source": [
        "## **9. Named Entity Recognition (NER) with spaCy**\n",
        "**Task:** Extract named entities from a complex sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc7654be",
      "metadata": {
        "id": "dc7654be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "519be496-be1b-4ca3-ac39-ed917b72803a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities:\n",
            "1969 -> DATE\n",
            "Neil Armstrong -> PERSON\n",
            "first -> ORDINAL\n",
            "Moon -> PERSON\n",
            "Apollo -> ORG\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"In 1969, Neil Armstrong became the first person to walk on the Moon during the Apollo 11 mission.\"\n",
        "\n",
        "doc = nlp(text)  # FILL THIS\n",
        "\n",
        "print(\"Named Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} -> {ent.label_}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}